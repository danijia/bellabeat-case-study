{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13316737,"sourceType":"datasetVersion","datasetId":8441908}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bellabeat Smart Device Usage Analysis\n### Case Study 2: How Can a Wellness Technology Company Play It Smart?\n\n**Author:** Danijia Haggins  \n**Date:** October 2025  \n**Tools:** Python, Pandas, Matplotlib, NumPy\n\n## **Scenario** \nI am a consumer insights analyst on the marketing analytics team at Bellabeat, a tech company that makes wellness products for women. The CCO of the company believes that analyzing smart device fitness data could provide valuable insights to inform Bellbeats marketing strategy. \n\n## **Business Task**:\nAnalyze smart device usage from FitBit users to discover trends in activity, sleep, and wellness habits. Apply these insights to help **Bellabeat** understand how consumers enage wit health-tracking devices. Apply these insights to help Bellabeat improve marketing strategies for the Leaf wellness tracker. \n\n[Data source](https://www.kaggle.com/datasets/arashnic/fitbit)\n\nIn this case study I am following 6 steps of the data analysis process: \n1. Ask\n2. Prepare\n3. Process\n4. Analyze\n5. Share\n6. Act \n","metadata":{}},{"cell_type":"markdown","source":"## ASK\n**Key stakeholders**:\n1. UrÅ¡ka SrÅ¡en (Cofounder & Chief Creative Officer)\n2. Sando Mur (Cofounder, Executive Team)\n\n**Key Questions Guiding Analysis**\n1. What are the main trends in smart device usage?\n2. How do these trends relate to Bellabeat customers?\n3. How can these insights inform Bellabeats marketing strategy?","metadata":{}},{"cell_type":"markdown","source":"## 1. Prepare\n\n### About this data\n* The data is being loaded, processed, and analyzed within Kaggle notebooks using python, stored for the purposes of this project memory\n* The time range this data represents is March 12, 2016-May 12, 2016\n* The source data is stored in CSVs (29 total)\n* The data is mostly long format, with a few of the csvs replicated in wide format\n* The data has been made available by Creative Commons under the [CCO: Public Domain License](https://creativecommons.org/publicdomain/zero/1.0/)","metadata":{}},{"cell_type":"code","source":"# installing libraries needed \n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt # plotting \nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T00:01:54.077494Z","iopub.execute_input":"2025-10-12T00:01:54.078815Z","iopub.status.idle":"2025-10-12T00:01:54.086107Z","shell.execute_reply.started":"2025-10-12T00:01:54.078769Z","shell.execute_reply":"2025-10-12T00:01:54.084785Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Process \n* I'm using python to process (clean/transform) because:\n    * I can clean, transform, and analyze the data in one place without switching platforms like with sql and excel.\n    * Python is also good for scalability as it can support high-volume data better than spreadsheets can (excel won't display more than 1,048,576 rows--this data includes more rows than that).\n    * I can document every step and analyze in one place, creating easily reproducible results.","metadata":{}},{"cell_type":"markdown","source":"### ðŸ“ˆðŸ›‘ Data Limitations\n* There are two folders containing data, one folder for user data between March 12, 2016 to April 11, 2016 and another for data from April 12, 2016 to May 12, 2016. Each folder contains 11 csvs with the same columns, and will need to be combined in the data cleaning/preprocessing.\n* Two participants (IDs 2891001357 and 6391747486) appear only in the first dailyActivity_merged dataset (from 3.12.16 to 4.11.16) and have no recorded activity after April 11, 1016. This may indicate device non-use or drop out during the study period. \n* Some csvs only represent a subset of data for users between April 12, 2016 - May 12, 2016: dailySteps, dailyintensities, dailycalories, and sleepday. This data is also redundant as total counts for steps and calories are included in the dailyActivities csvs, and steps are included in the minuteSteps csvs. We will not be using these csvs for the purposes of this analysis.\n* Some of the minute data is too granular for the purposes of this analysis: minuteCalories, minuteIntensities, and minuteMETS will not be used for the purposes of this analysis.\n*  Users may not wear their devices every day","metadata":{}},{"cell_type":"markdown","source":"### ðŸ“ Data Preparation \nOf the 29 csvs provided in the fitbit dataset, only the following were used for this analysis: \n* daily_activity\n* heartrate_seconds\n* minute_sleep\n* hourly_calories\n* hourly_intensities\n* hourly_steps\n* weight_logInfo \n\nThese were selected because they contain the most relevant and complete data for understanding user activity and health behavior. Files with incomplete, duplicate, or overly granular data (such as minute level logs) were excluded to simplify analysis and maintain clarity. ","metadata":{}},{"cell_type":"markdown","source":"### ðŸ”„ Data Loading\n* Import the csvs into dataframes \n* Inspect structure\n* Combine csvs with matching columns + data into one dataframe\n* Check dataframe data types to avoid computational errors. ","metadata":{}},{"cell_type":"code","source":"# loading the csv data into separate dataframes\nda1 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_3.12.16_to_4.11.16/dailyActivity_merged_1.csv')\nda2 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_4.12.16_to_5.12.16/dailyActivity_merged_2.csv')\nhs1 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_3.12.16_to_4.11.16/heartrate_seconds_merged_1.csv')\nhs2 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_4.12.16_to_5.12.16/heartrate_seconds_merged_2.csv')\nhc1 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_3.12.16_to_4.11.16/hourlyCalories_merged_1.csv')\nhc2 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_4.12.16_to_5.12.16/hourlyCalories_merged_2.csv')\nhi1 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_3.12.16_to_4.11.16/hourlyIntensities_merged_1.csv')\nhi2 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_4.12.16_to_5.12.16/hourlyIntensities_merged_2.csv')\nhstps1 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_3.12.16_to_4.11.16/hourlySteps_merged_1.csv')\nhstps2 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_4.12.16_to_5.12.16/hourlySteps_merged_2.csv')\nmsl1= pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_3.12.16_to_4.11.16/minuteSleep_merged_1.csv')\nmsl2 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_4.12.16_to_5.12.16/minuteSleep_merged_2.csv')\nwlg1 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_3.12.16_to_4.11.16/weightLogInfo_merged_1.csv') \nwlg2 = pd.read_csv('/kaggle/input/bella-beat-case-study/data/fitabase_data_4.12.16_to_5.12.16/weightLogInfo_merged_2.csv')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:00:35.725499Z","iopub.execute_input":"2025-10-11T23:00:35.725970Z","iopub.status.idle":"2025-10-11T23:00:41.769096Z","shell.execute_reply.started":"2025-10-11T23:00:35.725946Z","shell.execute_reply":"2025-10-11T23:00:41.767800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inspecting dataframes structure\n\n# store data frames loaded above in a list \ndfs = [da1, da2, hs1, hs2, hc1, hc2, hi1, hi2, hstps1, hstps2, msl1, msl2, wlg1, wlg2]\n\n# iterate over the list with a loop\n# enumerate(iterable, start=0)\nfor i, x in enumerate(dfs, 1): # i = counter, start=1, x = df from dfs list\n    print(f\"Dataframe {i}:\") # print number of the dataframe\n    display(x.head(2)) # use display() for notebook friendly display\n    print(\"\\n\") # add space between outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:00:45.201458Z","iopub.execute_input":"2025-10-11T23:00:45.201806Z","iopub.status.idle":"2025-10-11T23:00:45.342829Z","shell.execute_reply.started":"2025-10-11T23:00:45.201783Z","shell.execute_reply":"2025-10-11T23:00:45.340689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check for nulls / missing data\n# create a dictionary with the dataframes and their names\ndict1 = {'da1': da1, \n       'da2': da2, \n       'hs1': hs1, \n       'hs2': hs2, \n       'hc1': hc1, \n       'hc2': hc2, \n       'hi1': hi1, \n       'hi2': hi2, \n       'hstps1': hstps1, \n       'hstps': hstps2,  \n       'msl1': msl1, \n       'msl2': msl2, \n       'wlg1': wlg1, \n       'wlg2': wlg2\n      }\n\n# iterate over dict1\nfor name, df in dict1.items():\n    print(f\"Null values in {name}:\")\n    null_counts = df.isnull().sum()\n    if null_counts.sum() > 0: # can use .any() or .sum()\n        print(null_counts[null_counts > 0]) # Print only columns with nulls\n    else:\n        print(\"No null values found.\")\n    print(\"\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:00:59.811323Z","iopub.execute_input":"2025-10-11T23:00:59.811692Z","iopub.status.idle":"2025-10-11T23:01:00.081640Z","shell.execute_reply.started":"2025-10-11T23:00:59.811666Z","shell.execute_reply":"2025-10-11T23:01:00.080269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note:** \nDatframes wlg1 & wlg2 contain NaN values in the body fat column because some users did not record their body fat percentages. This will not cause errors in analysis. Leaving this as is but making a note here. ","metadata":{}},{"cell_type":"code","source":"# combining csv's with the same columns into one dataframe\ndaily_activity = pd.concat([da1, da2])\nheartrate_seconds = pd.concat([hs1, hs2])\nhourly_calories = pd.concat([hc1, hc2])\nhourly_intensities = pd.concat([hi1, hi2])\nhourly_steps = pd.concat([hstps1, hstps2])\nminute_sleep = pd.concat([msl1, msl2])\nweight_log_info = pd.concat([wlg1, wlg2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:01:05.089052Z","iopub.execute_input":"2025-10-11T23:01:05.089476Z","iopub.status.idle":"2025-10-11T23:01:05.317356Z","shell.execute_reply.started":"2025-10-11T23:01:05.089449Z","shell.execute_reply":"2025-10-11T23:01:05.316298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# store combined + other dataframes in a dataframe\nall_dfs = [daily_activity, heartrate_seconds, hourly_calories, hourly_intensities, hourly_steps, minute_sleep, weight_log_info]\n\n# view first 5 rows of each dataframe\nfor i, x in enumerate(all_dfs, 1): \n    print(f'Dataframe {i}:')\n    display(x.head())\n    print('\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:01:10.562396Z","iopub.execute_input":"2025-10-11T23:01:10.562765Z","iopub.status.idle":"2025-10-11T23:01:10.622607Z","shell.execute_reply.started":"2025-10-11T23:01:10.562713Z","shell.execute_reply":"2025-10-11T23:01:10.621492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking data types\nall_dfs[0].info() # daily_activity, position 0 in all_dfs \nall_dfs[1].info() # heartrate seconds, position 1 in all_dfs\nall_dfs[2].info() # hourly calories, position 2 in all_dfs\nall_dfs[3].info() # hourly intensities, position 3 in all_dfs\nall_dfs[4].info() # hourly steps, position 4 in all_dfs\nall_dfs[5].info() # minute sleep, position 5 in all_dfs\nall_dfs[6].info() # weight_log_info, position 6 in all_dfs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:01:17.033516Z","iopub.execute_input":"2025-10-11T23:01:17.035480Z","iopub.status.idle":"2025-10-11T23:01:17.137507Z","shell.execute_reply.started":"2025-10-11T23:01:17.035431Z","shell.execute_reply":"2025-10-11T23:01:17.135976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert datetime columns from objects to datetime64 datatypes & check the columns and datatypes\ndaily_activity['ActivityDate'] = pd.to_datetime(daily_activity['ActivityDate'])\nheartrate_seconds['Time'] = pd.to_datetime(heartrate_seconds['Time'])\nhourly_calories['ActivityHour'] = pd.to_datetime(hourly_calories['ActivityHour'])\nhourly_intensities['ActivityHour'] = pd.to_datetime(hourly_intensities['ActivityHour'])\nhourly_steps['ActivityHour'] = pd.to_datetime(hourly_steps['ActivityHour'])\nminute_sleep['date'] = pd.to_datetime(minute_sleep['date'])\nweight_log_info['Date'] = pd.to_datetime(weight_log_info['Date'])\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:02:03.224150Z","iopub.execute_input":"2025-10-11T23:02:03.224492Z","iopub.status.idle":"2025-10-11T23:02:03.360865Z","shell.execute_reply.started":"2025-10-11T23:02:03.224468Z","shell.execute_reply":"2025-10-11T23:02:03.359373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check that datetime columns changed to datetime64\ndaily_activity['ActivityDate'].info() # daily_activity still an object but thats fine i think\nheartrate_seconds['Time'].info() # heartrate seconds\nhourly_calories['ActivityHour'].info() # hourly calories\nhourly_intensities['ActivityHour'].info() # hourly intensities\nhourly_steps['ActivityHour'] .info() # hourly steps\nminute_sleep['date'].info() # minute sleep\nweight_log_info['Date'].info() # weight_log_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:02:05.615996Z","iopub.execute_input":"2025-10-11T23:02:05.616326Z","iopub.status.idle":"2025-10-11T23:02:05.678026Z","shell.execute_reply.started":"2025-10-11T23:02:05.616304Z","shell.execute_reply":"2025-10-11T23:02:05.676684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking dates: \n\n# view first 5 rows of each dataframe\nfor i, x in enumerate(all_dfs, 1): \n    print(f'Dataframe {i}:')\n    display(x.head())\n    print('\\n')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:02:09.270425Z","iopub.execute_input":"2025-10-11T23:02:09.270804Z","iopub.status.idle":"2025-10-11T23:02:09.340616Z","shell.execute_reply.started":"2025-10-11T23:02:09.270775Z","shell.execute_reply":"2025-10-11T23:02:09.339563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Analyze \nNow that data has be loaded and preprocessed, it's time for analysis. \n\n**Guiding questions:**\n* What suprises did you find in the data?\n* What trends and relationships did you find in the data?\n\n**Key Tasks**\n* Aggregate data so it's useful and accessible\n* Organize and format data\n* Perform calculations","metadata":{}},{"cell_type":"code","source":"# start with daily_activities dataframe\n\ndaily_activity.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:02:15.857848Z","iopub.execute_input":"2025-10-11T23:02:15.858175Z","iopub.status.idle":"2025-10-11T23:02:15.873673Z","shell.execute_reply.started":"2025-10-11T23:02:15.858152Z","shell.execute_reply":"2025-10-11T23:02:15.872595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(daily_activity['ActivityDate'].min())\ndisplay(daily_activity.sort_values('ActivityDate', ascending=True).head(10))\n\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:02:22.498659Z","iopub.execute_input":"2025-10-11T23:02:22.499311Z","iopub.status.idle":"2025-10-11T23:02:22.529964Z","shell.execute_reply.started":"2025-10-11T23:02:22.499191Z","shell.execute_reply":"2025-10-11T23:02:22.528831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(daily_activity['Id'].nunique()) # 35 unique ids\nunique_ids = daily_activity.Id.unique()\nprint(type(unique_ids))\nunique_ids = unique_ids.tolist()\nprint(type(unique_ids))\nprint(unique_ids)\ndisplay(daily_activity[daily_activity['Id'] == 2891001357].sort_values('ActivityDate', ascending=True)) \ndisplay(daily_activity[daily_activity['Id'] == 6391747486].sort_values('ActivityDate', ascending=True))\n# display(daily_activity[daily_activity.TotalSteps == 0].sort_values('ActivityDate', ascending=True).reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:02:29.682598Z","iopub.execute_input":"2025-10-11T23:02:29.683485Z","iopub.status.idle":"2025-10-11T23:02:29.723037Z","shell.execute_reply.started":"2025-10-11T23:02:29.683449Z","shell.execute_reply":"2025-10-11T23:02:29.721960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# calculate aggregates\n\n# avg steps per user \navg_steps_per_user = daily_activity.groupby('Id').TotalSteps.mean()\navg_steps_per_user = avg_steps_per_user.to_frame().sort_values('TotalSteps', ascending=True)\nprint(type(avg_steps_per_user))\n\nprint(avg_steps_per_user)\n\n# user with total average steps = 8877689391\n\n# sum of all steps per user\ntotal_steps_per_user = daily_activity.groupby('Id').TotalSteps.sum()\n\ntotal_steps_per_user = total_steps_per_user.to_frame().sort_values('TotalSteps', ascending=True)\n\nprint(total_steps_per_user)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T23:30:52.407656Z","iopub.execute_input":"2025-10-11T23:30:52.408375Z","iopub.status.idle":"2025-10-11T23:30:52.420036Z","shell.execute_reply.started":"2025-10-11T23:30:52.408348Z","shell.execute_reply":"2025-10-11T23:30:52.418200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# find out what days people took the most steps?\n# pd.set_option('display.max_rows', None)\ntotal_steps_by_day = daily_activity.groupby('ActivityDate').TotalSteps.sum()\ntotal_steps_by_day = total_steps_by_day.to_frame().sort_values('TotalSteps', ascending=True)\nprint(total_steps_by_day) # 2016-04-12 had the most steps = 314095 -- the last day of the data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T00:20:35.492223Z","iopub.execute_input":"2025-10-12T00:20:35.492585Z","iopub.status.idle":"2025-10-12T00:20:35.506599Z","shell.execute_reply.started":"2025-10-12T00:20:35.492561Z","shell.execute_reply":"2025-10-12T00:20:35.504892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum_steps_calories_per_user = daily_activity.groupby('Id').agg(\n    TotalCalories = ('Calories', 'sum'), \n    TotalSteps = ('TotalSteps', 'sum')\n)\n\nprint(sum_steps_calories_per_user)\n# calories vs. steps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T00:47:41.080375Z","iopub.execute_input":"2025-10-12T00:47:41.081381Z","iopub.status.idle":"2025-10-12T00:47:41.097242Z","shell.execute_reply.started":"2025-10-12T00:47:41.081212Z","shell.execute_reply":"2025-10-12T00:47:41.096184Z"}},"outputs":[],"execution_count":null}]}